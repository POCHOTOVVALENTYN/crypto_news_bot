# parser/rss_parser.py
import feedparser
import aiohttp
import asyncio
import re
from typing import List, Dict
from html import unescape

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = {
    "Forklog": "https://forklog.com/feed/",
    "Bits.media": "https://bits.media/feed/",
}

# –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (fallback)
ENGLISH_FEEDS = {
    "CoinDesk": "https://www.coindesk.com/feed",
    "Cointelegraph": "https://cointelegraph.com/rss",
    "Decrypt": "https://decrypt.co/feed",
}

WHITELIST_KEYWORDS = [
    "bitcoin", "ethereum", "btc", "eth", "crypto", "blockchain",
    "sec", "regulation", "trading", "market", "price", "exchange",
    "ripple", "xrp", "solana", "cardano", "polygon", "bnb", "usdt",
    "–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–π–Ω", "—ç—Ñ–∏—Ä–∏—É–º", "–±–ª–æ–∫—á–µ–π–Ω", "—Ç–æ—Ä–≥–æ–≤–ª—è",
    "—Ä—ã–Ω–æ–∫", "—Ü–µ–Ω–∞", "–æ–±–º–µ–Ω", "—Ä–µ–≥—É–ª—è—Ü–∏—è", "–º–∞–π–Ω–∏–Ω–≥",
]

BLACKLIST_KEYWORDS = [
    "nft collection", "airdrop", "presale", "promo", "giveaway",
    "casino", "gambling", "lottery", "scam",
    "–≥–∏–≤—ç–≤–µ–π", "–∫–∞–∑–∏–Ω–æ", "–ª–æ—Ç–µ—Ä–µ—è", "–ø–∞–º–ø–∏–Ω–≥", "—Å—Ö–µ–º–∞",
]

# ‚úÖ –ù–û–í–û–ï: –°–ª–æ–≤–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
REMOVE_KEYWORDS = [
    "–∏—Å—Ç–æ—á–Ω–∏–∫:", "–¥–∂–µ—Ä–µ–ª–æ:", "source:", "via:", "read more:",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ:", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ:", "—á–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é:",
    "cryptoquant", "glassnode", "coindesk", "cointelegraph",
    "forklog", "bits.media", "cryptonuz", "miningcrypto",
]


def clean_html(text: str) -> str:
    """–£–¥–∞–ª–∏—Ç–µ HTML —Ç–µ–≥–∏ –∏ —Ä–∞—Å—à–∏—Ñ—Ä—É–π—Ç–µ HTML —Å—É—â–Ω–æ—Å—Ç–∏"""
    text = re.sub(r'<[^>]+>', '', text)
    text = unescape(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def remove_source_mentions(text: str) -> str:
    """
    ‚úÖ –ù–û–í–û–ï: –£–¥–∞–ª–∏—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞

    –ü—Ä–∏–º–µ—Ä—ã:
    - "–ò—Å—Ç–æ—á–Ω–∏–∫: CryptoQuant" ‚Üí ""
    - "Source: Bloomberg" ‚Üí ""
    - "via CoinDesk" ‚Üí ""
    """
    text_lower = text.lower()

    # –ù–∞–π–¥–∏—Ç–µ –ø–æ–∑–∏—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    min_position = len(text)

    for keyword in REMOVE_KEYWORDS:
        pos = text_lower.find(keyword.lower())
        if pos != -1 and pos < min_position:
            min_position = pos

    # –û–±—Ä–µ–∂—å—Ç–µ —Ç–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if min_position < len(text):
        text = text[:min_position].strip()

    # –£–¥–∞–ª–∏—Ç–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç—ã–µ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
    text = text.rstrip('.,;: ')

    return text


class RSSParser:
    def __init__(self, use_russian: bool = True):
        self.feeds = RSS_FEEDS if use_russian else ENGLISH_FEEDS
        self.use_russian = use_russian

    @staticmethod
    def _is_relevant(title: str, description: str = "") -> bool:
        """–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"""
        text = (title + " " + description).lower()

        # –°–Ω–∞—á–∞–ª–∞ blacklist
        for keyword in BLACKLIST_KEYWORDS:
            if keyword in text:
                return False

        # –ó–∞—Ç–µ–º whitelist
        for keyword in WHITELIST_KEYWORDS:
            if keyword in text:
                return True

        return False

    @staticmethod
    def _detect_language(text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
        if re.search(r'[–∞-—è–ê-–Ø—ë–Å]', text):
            return "ru"
        return "en"

    @staticmethod
    def _extract_image_from_entry(entry: dict) -> str:
        """–ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ entry"""
        try:
            # 1. media_content
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if 'url' in media:
                        return media['url']

            # 2. enclosures
            if hasattr(entry, 'enclosures') and entry.enclosures:
                for enc in entry.enclosures:
                    if enc.get('type', '').startswith('image'):
                        return enc.get('href')

            # 3. links
            if hasattr(entry, 'links') and entry.links:
                for link in entry.links:
                    link_type = link.get('type', '')
                    if 'image' in link_type or link.get('rel') == 'image':
                        return link.get('href')

            # 4. summary (HTML img tag)
            if hasattr(entry, 'summary') and entry.summary:
                img_urls = re.findall(
                    r'<img[^>]+src=["\']([^"\']+)["\']',
                    entry.summary
                )
                if img_urls:
                    return img_urls[0]

            # 5. image –ø–æ–ª–µ
            if hasattr(entry, 'image'):
                if isinstance(entry.image, dict):
                    return entry.image.get('href') or entry.image.get('url')
                elif isinstance(entry.image, str):
                    return entry.image

            # 6. description
            if hasattr(entry, 'description') and entry.description:
                img_urls = re.findall(
                    r'<img[^>]+src=["\']([^"\']+)["\']',
                    entry.description
                )
                if img_urls:
                    return img_urls[0]

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

        return None

    async def fetch_feed(self, feed_url: str) -> List[dict]:
        """–ü–∞—Ä—Å—å—Ç–µ RSS –ª–µ–Ω—Ç—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                        feed_url,
                        timeout=aiohttp.ClientTimeout(total=20)  # ‚úÖ –£–≤–µ–ª–∏—á–µ–Ω –¥–æ 20 —Å–µ–∫—É–Ω–¥
                ) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        feed = feedparser.parse(content)
                        return feed.entries[:20]
                    else:
                        print(f"‚ö†Ô∏è HTTP {resp.status}: {feed_url}")
        except asyncio.TimeoutError:
            print(f"‚è±Ô∏è Timeout (20s): {feed_url}")
        except aiohttp.ClientConnectorError as e:
            print(f"üîå Connection error: {feed_url}")
        except Exception as e:
            print(f"‚ùå Unexpected error: {feed_url}: {e}")

        return []

    async def get_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []

        for source_name, feed_url in self.feeds.items():
            print(f"üîÑ Fetching: {source_name}...")
            entries = await self.fetch_feed(feed_url)

            if not entries:
                print(f"‚ö†Ô∏è No entries from {source_name}")
                continue

            print(f"‚úÖ Found {len(entries)} entries from {source_name}")

            for entry in entries:
                title = entry.get("title", "No title")
                link = entry.get("link", "")
                published = entry.get("published", "")
                summary = entry.get("summary", "")

                # –û—á–∏—Å—Ç–∏—Ç–µ HTML
                summary = clean_html(summary)

                # ‚úÖ –ù–û–í–û–ï: –£–¥–∞–ª–∏—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                summary = remove_source_mentions(summary)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if not self._is_relevant(title, summary):
                    continue

                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —è–∑—ã–∫
                lang = self._detect_language(title + " " + summary)

                # –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = self._extract_image_from_entry(entry)

                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª–Ω—ã–π summary –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è
                all_news.append({
                    "title": title,
                    "link": link,
                    "source": source_name,
                    "published": published,
                    "summary": summary,  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                    "language": lang,
                    "image_url": image_url,
                    "raw_entry": entry,
                })

        return all_news
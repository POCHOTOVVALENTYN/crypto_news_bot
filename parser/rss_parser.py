# parser/rss_parser.py
import feedparser
import aiohttp
import asyncio
import re
from typing import List, Dict
from html import unescape

# ‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –¢–û–õ–¨–ö–û –†–ê–ë–û–ß–ò–ï
RSS_FEEDS = {
    "Forklog": "https://forklog.com/feed/",
    "Coinspot": "https://coinspot.io/feed/",
}

TIER_1_FEEDS = {
    "CoinDesk": "https://www.coindesk.com/arc/outboundfeeds/rss/",
    "Cointelegraph": "https://cointelegraph.com/rss",
    "Decrypt": "https://decrypt.co/feed",
    "The Block": "https://www.theblock.co/rss.xml",
}

# ‚úÖ WHITELIST —Ä–∞—Å—à–∏—Ä–µ–Ω
WHITELIST_KEYWORDS = [
    # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã
    "bitcoin", "ethereum", "btc", "eth", "crypto", "blockchain",
    "solana", "cardano", "polygon", "bnb", "usdt", "usdc",
    "ripple", "xrp", "doge", "dogecoin", "shib", "ada", "dot",

    # –†–µ–≥—É–ª—è—Ü–∏—è
    "sec", "regulation", "—Ä–µ–≥—É–ª—è—Ü–∏—è", "–∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
    "trump", "—Ç—Ä–∞–º–ø", "biden", "–±–∞–π–¥–µ–Ω", "congress", "–∫–æ–Ω–≥—Ä–µ—Å—Å",

    # –ö–æ–º–ø–∞–Ω–∏–∏
    "coinbase", "binance", "bybit", "okx", "kraken",
    "microstrategy", "tesla", "blackrock", "grayscale",

    # –°–æ–±—ã—Ç–∏—è
    "etf", "listing", "–ª–∏—Å—Ç–∏–Ω–≥", "hack", "–≤–∑–ª–æ–º",
    "trading", "—Ç–æ—Ä–≥–æ–≤–ª—è", "market", "—Ä—ã–Ω–æ–∫", "price", "—Ü–µ–Ω–∞",

    # –†—É—Å—Å–∫–∏–π
    "–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–π–Ω", "—ç—Ñ–∏—Ä–∏—É–º", "–±–ª–æ–∫—á–µ–π–Ω",
    "–±–∏—Ä–∂–∞", "–æ–±–º–µ–Ω", "–º–∞–π–Ω–∏–Ω–≥",
]

BLACKLIST_KEYWORDS = [
    "nft collection", "airdrop", "presale", "promo", "giveaway",
    "casino", "gambling", "lottery", "scam", "ponzi",
    "–≥–∏–≤—ç–≤–µ–π", "–∫–∞–∑–∏–Ω–æ", "–ª–æ—Ç–µ—Ä–µ—è", "—Å—Ö–µ–º–∞", "—Ä–∞–∑–≤–æ–¥",
]

REMOVE_KEYWORDS = [
    "–∏—Å—Ç–æ—á–Ω–∏–∫:", "–¥–∂–µ—Ä–µ–ª–æ:", "source:", "via:", "read more:",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ:", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ:", "—á–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é:",
    "cryptoquant", "glassnode", "coindesk", "cointelegraph",
    "forklog", "bits.media", "rbc", "coinspot",
]


def clean_html(text: str) -> str:
    """–£–¥–∞–ª–∏—Ç–µ HTML —Ç–µ–≥–∏ –∏ —Ä–∞—Å—à–∏—Ñ—Ä—É–π—Ç–µ HTML —Å—É—â–Ω–æ—Å—Ç–∏"""
    text = re.sub(r'<[^>]+>', '', text)
    text = unescape(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def remove_source_mentions(text: str) -> str:
    """–£–¥–∞–ª–∏—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    min_position = len(text)

    for keyword in REMOVE_KEYWORDS:
        pos = text_lower.find(keyword.lower())
        if pos != -1 and pos < min_position:
            min_position = pos

    if min_position < len(text):
        text = text[:min_position].strip()

    text = text.rstrip('.,;: ')
    return text


class RSSParser:
    def __init__(self, use_russian: bool = True, include_tier1: bool = True):
        """
        use_russian: —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        include_tier1: –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–º–∏—É–º –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ
        """
        self.feeds = {}

        if use_russian:
            self.feeds.update(RSS_FEEDS)

        if include_tier1:
            self.feeds.update(TIER_1_FEEDS)

    @staticmethod
    def _is_relevant(title: str, description: str = "") -> bool:
        """–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"""
        text = (title + " " + description).lower()

        # Blacklist
        for keyword in BLACKLIST_KEYWORDS:
            if keyword in text:
                return False

        # Whitelist
        for keyword in WHITELIST_KEYWORDS:
            if keyword in text:
                return True

        return False

    @staticmethod
    def _detect_language(text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
        if re.search(r'[–∞-—è–ê-–Ø—ë–Å]', text):
            return "ru"
        return "en"

    @staticmethod
    def _extract_image_from_entry(entry: dict) -> str:
        """–ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ entry"""
        try:
            # 1. media_content
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if 'url' in media:
                        return media['url']

            # 2. enclosures
            if hasattr(entry, 'enclosures') and entry.enclosures:
                for enc in entry.enclosures:
                    if enc.get('type', '').startswith('image'):
                        return enc.get('href')

            # 3. links
            if hasattr(entry, 'links') and entry.links:
                for link in entry.links:
                    link_type = link.get('type', '')
                    if 'image' in link_type or link.get('rel') == 'image':
                        return link.get('href')

            # 4. summary (HTML img tag)
            if hasattr(entry, 'summary') and entry.summary:
                img_urls = re.findall(
                    r'<img[^>]+src=["\']([^"\']+)["\']',
                    entry.summary
                )
                if img_urls:
                    return img_urls[0]

            # 5. image –ø–æ–ª–µ
            if hasattr(entry, 'image'):
                if isinstance(entry.image, dict):
                    return entry.image.get('href') or entry.image.get('url')
                elif isinstance(entry.image, str):
                    return entry.image

            # 6. description
            if hasattr(entry, 'description') and entry.description:
                img_urls = re.findall(
                    r'<img[^>]+src=["\']([^"\']+)["\']',
                    entry.description
                )
                if img_urls:
                    return img_urls[0]

        except Exception as e:
            pass

        return None

    async def fetch_feed(self, feed_url: str) -> List[dict]:
        """–ü–∞—Ä—Å—å—Ç–µ RSS –ª–µ–Ω—Ç—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            # ‚úÖ –î–û–ë–ê–í–õ–ï–ù User-Agent (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∞–π—Ç—ã –±–ª–æ–∫–∏—Ä—É—é—Ç –±–µ–∑ –Ω–µ–≥–æ)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(
                        feed_url,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=20)
                ) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        feed = feedparser.parse(content)
                        return feed.entries[:20]
                    else:
                        print(f"‚ö†Ô∏è HTTP {resp.status}: {feed_url}")

        except asyncio.TimeoutError:
            print(f"‚è±Ô∏è Timeout (20s): {feed_url}")
        except aiohttp.ClientConnectorError:
            print(f"üîå Connection error: {feed_url}")
        except Exception as e:
            print(f"‚ùå Error: {feed_url}: {e}")

        return []

    async def get_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []

        for source_name, feed_url in self.feeds.items():
            print(f"üîÑ Fetching: {source_name}...")
            entries = await self.fetch_feed(feed_url)

            if not entries:
                print(f"‚ö†Ô∏è No entries from {source_name}")
                continue

            print(f"‚úÖ Found {len(entries)} entries from {source_name}")

            for entry in entries:
                title = entry.get("title", "No title")
                link = entry.get("link", "")
                published = entry.get("published", "")
                summary = entry.get("summary", "")

                summary = clean_html(summary)
                summary = remove_source_mentions(summary)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if not self._is_relevant(title, summary):
                    continue

                lang = self._detect_language(title + " " + summary)
                image_url = self._extract_image_from_entry(entry)

                all_news.append({
                    "title": title,
                    "link": link,
                    "source": source_name,
                    "published": published,
                    "summary": summary,
                    "language": lang,
                    "image_url": image_url,
                    "raw_entry": entry,
                })

        return all_news